实习期间老板需要整合土地资源数据。之前由于都是在[中国土地网](https://www.landchina.com/)上对每个城市的数据进行爬取，但是出于精益求精的精神，决定再对每个城市的自然资源和规划局再次进行爬取，这样可以对数据进行比对。或者添加新数据或者去重。

其实每个城市的自然资源和规划局网站结构基本类似，本次我们就拿徐州市来做一个案例分析。
#### 爬取目标
如图所示。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020061316385481.png#pic_center)
我们爬取字段如下：

* 行政区
* 项目名称
* 项目位置
* 合同编号
* 电子监管 
* 面积_公顷
* 土地来源
* 供地方式
* 土地使用年限
* 行业分类 
* 土地用途
* 土地级别
* 成交价格_万元
* 土地使用权人
* 约定容积率_下限
* 约定容积率_上限
* 约定交地时间
* 约定开工时间
* 约定竣工时间
* 批准单位
* 签订日期

#### 网页分析
首先我们打开[官网](http://zrzy.jiangsu.gov.cn/xz/)
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200613164213250.png#pic_center)
接着向下滑动网页，找到土地市场，然后点击进去。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020061316430741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNTc3MDA2Nw==,size_16,color_FFFFFF,t_70#pic_center)
接着跳转到新的链接，我们依次点击土地市场，供地结果，下面出来的招拍、协议出让、划拨用地就是我们此次需要爬取的三个内容。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200613164409732.png#pic_center)
我们先点击招拍挂出结果公告，右边就显示出项目名称。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200613164604735.png#pic_center)
我们点击进去项目名称，然后就是各个土地的拍卖和公司信息。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2020061316470439.png#pic_center)
看起来逻辑很简单，我们只需要先对项目名称的链接进行爬取，然后根据爬取到的链接再进去爬取具体的内容。

但是我们在对一个具体的链接使用requests库进行测试的时候，发现返回的只是基础的网页内容，不涉及到具体的数据。这时候就知道，前后端是分离的，我们需要找到真正的链接。

通过不断查找，我们发现真正的项目链接竟然隐藏在表格的iframe的链接中。这样这样就找到了真正的链接，依次进行爬取。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200613165044889.png#pic_center)
#### 爬取内容
首先我们先爬取项目名称的链接。我们将获取到的链接保存到txt文件中，方面后面调用。

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from pyquery import PyQuery as pq
import time

browser=webdriver.Chrome()
wait=WebDriverWait(browser,10)

def search():
    browser.get('http://zrzy.jiangsu.gov.cn//gtapp/xxgk/tdsc_getGdxminfo.action?xzqhdm=320300&xsys=1&lx=1')
    get_products()

def next_page():
    submit = wait.until(
            EC.element_to_be_clickable((By.XPATH, '//*[@id="gdxmPagingHtml"]/a[3]')))
    submit.click()
    get_products()

ss=[]
def get_products():
    html=browser.page_source
    doc=pq(html)
    #print(doc)
    comments = []
    items=doc('.xxgk-listz #gdxmTableHtml .taleft1').items()
    for item in items:
        lis = {}
        lis['link']='http://zrzy.jiangsu.gov.cn/gtapp/xxgk/'+item.find('a').attr('href')
        comments.append(lis)
    with open('划拨用地.txt', 'a+') as f:
        for comment in comments:
            f.write('{} \n'.format(
                comment['link']))

        print('当前页面爬取完成')

def main():
    print("正在输出第1页")
    search()
    for i in range(2,43):
        time.sleep(1)
        print("正在输出第{}页".format(i))
        next_page()
    browser.close()


if __name__ == '__main__':
    main()
```
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200613165358419.png#pic_center)
接着我们读取txt中的链接，分别进入到每个网页对具体内容进行爬取，并保存到CSV文件中。

```python
# coding='utf-8'
import requests
from bs4 import BeautifulSoup
import time
import csv

# 首先我们写好抓取网页的函数
def get_html(url):
    try:
        r = requests.get(url,timeout=30)
        r.raise_for_status()
        r.encoding='utf-8'
        return r.text
    except:
        return " ERROR "

def get_content(url):
    #global values
    values = []
    html=get_html(url)
    soup=BeautifulSoup(html,'lxml')
    行政区=soup.find('span',attrs={'id':'XZQ_DM'}).text.strip()
    项目名称=soup.find('span', attrs={'id': 'XM_MC'}).text.strip()
    项目位置 = soup.find('span', attrs={'id': 'TD_ZL'}).text.strip()
    合同编号 = soup.find('span', attrs={'id': 'BH'}).text.strip()
    电子监管 = soup.find('span', attrs={'id': 'DZ_BA_BH'}).text.strip()
    面积_公顷= soup.find('span', attrs={'id': 'GY_MJ'}).text.strip()
    土地来源 = soup.find('span', attrs={'id': 'TD_LY'}).text.strip()
    供地方式=soup.find('span', attrs={'id': 'GY_FS'}).text.strip()
    土地使用年限 = soup.find('span', attrs={'id': 'CR_NX'}).text.strip()
    行业分类 = soup.find('span', attrs={'id': 'HY_FL'}).text.strip()
    土地用途= soup.find('span', attrs={'id': 'TD_YT'}).text.strip()
    土地级别= soup.find('span', attrs={'id': 'TD_JB'}).text.strip()
    成交价格_万元= soup.find('span', attrs={'id': 'JE'}).text.strip()
    土地使用权人= soup.find('span', attrs={'id': 'SRR'}).text.strip()
    约定容积率_下限= soup.find('span', attrs={'id': 'MIN_RJL'}).text.strip()
    约定容积率_上限= soup.find('span', attrs={'id': 'MAX_RJL'}).text.strip()
    约定交地时间= soup.find('span', attrs={'id': 'JD_SJ'}).text.strip()
    约定开工时间= soup.find('span', attrs={'id': 'DG_SJ'}).text.strip()
    约定竣工时间 = soup.find('span', attrs={'id': 'JG_SJ'}).text.strip()
    批准单位= soup.find('span', attrs={'id': 'PZ_JG'}).text.strip()
    签订日期= soup.find('span', attrs={'id': 'QD_RQ'}).text.strip()

    lis=[行政区,项目名称,项目位置,合同编号 ,电子监管 ,面积_公顷,土地来源,供地方式,土地使用年限 ,行业分类 ,土地用途,土地级别,成交价格_万元,土地使用权人,约定容积率_下限,约定容积率_上限,约定交地时间,约定开工时间,约定竣工时间,批准单位,签订日期]
    values.append(lis)
    return values


def outfile(values):
    with open("xuzhou_huabo.csv","a+",newline='',encoding='utf8')as csvfile:
        writer = csv.writer(csvfile)
        #writer.writerow(['行政区','项目名称','项目位置','合同编号' ,'电子监管' ,'面积_公顷','土地来源','供地方式','土地使用年限' ,'行业分类' ,'土地用途','土地级别','成交价格_万元','土地使用权人','约定容积率_下限','约定容积率_上限','约定交地时间','约定开工时间','约定竣工时间','批准单位','签订日期'])
        #for row in values:
            #writer.writerow(row)
        writer.writerows(values)


def main():
    with open('划拨用地.txt', 'r') as f:
        count=0
        for line in f:
            count+=1
            print("正在解析第{}个页面，一共有756个页面".format(count))
            time.sleep(1)
            values=get_content(line)
            outfile(values)
            print("所有信息都已经爬取完成")

if __name__ == '__main__':
    main()
```
也可以将抓取到的数据保存到txt文件中。

```python
# coding='utf-8'
import requests
from bs4 import BeautifulSoup
import time

def get_html(url):
    try:
        r = requests.get(url,timeout=30)
        r.raise_for_status()
        r.encoding='utf-8'
        return r.text
    except:
        return " ERROR "

def get_content(url):
    comments=[]
    html=get_html(url)
    soup=BeautifulSoup(html,'lxml')
    comment={}
    comment['行政区']=soup.find('span',attrs={'id':'XZQ_DM'}).text.strip()
    comment['项目名称'] =soup.find('span', attrs={'id': 'XM_MC'}).text.strip()
    comment['项目位置'] = soup.find('span', attrs={'id': 'TD_ZL'}).text.strip()
    comment['合同编号'] = soup.find('span', attrs={'id': 'BH'}).text.strip()
    comment['电子监管号'] = soup.find('span', attrs={'id': 'DZ_BA_BH'}).text.strip()
    comment['面积(公顷)'] = soup.find('span', attrs={'id': 'GY_MJ'}).text.strip()
    comment['土地来源'] = soup.find('span', attrs={'id': 'TD_LY'}).text.strip()
    comment['供地方式']=soup.find('span', attrs={'id': 'GY_FS'}).text.strip()
    comment['土地使用年限'] = soup.find('span', attrs={'id': 'CR_NX'}).text.strip()
    comment['行业分类'] = soup.find('span', attrs={'id': 'HY_FL'}).text.strip()
    comment['土地用途'] = soup.find('span', attrs={'id': 'TD_YT'}).text.strip()
    comment['土地级别'] = soup.find('span', attrs={'id': 'TD_JB'}).text.strip()
    comment['成交价格(万元)'] = soup.find('span', attrs={'id': 'JE'}).text.strip()
    comment['土地使用权人'] = soup.find('span', attrs={'id': 'SRR'}).text.strip()
    comment['约定容积率(下限)'] = soup.find('span', attrs={'id': 'MIN_RJL'}).text.strip()
    comment['约定容积率(上限)'] = soup.find('span', attrs={'id': 'MAX_RJL'}).text.strip()
    comment['约定交地时间'] = soup.find('span', attrs={'id': 'JD_SJ'}).text.strip()
    comment['约定开工时间'] = soup.find('span', attrs={'id': 'DG_SJ'}).text.strip()
    comment['约定竣工时间'] = soup.find('span', attrs={'id': 'JG_SJ'}).text.strip()
    comment['批准单位'] = soup.find('span', attrs={'id': 'PZ_JG'}).text.strip()
    comment['签订日期'] = soup.find('span', attrs={'id': 'QD_RQ'}).text.strip()
    comments.append(comment)
    return comments



def outfile(dict):
    with open('xuzhou.txt','a+',encoding='utf-8')as f:
        for comment in dict:
            f.write('行政区:{} \t 项目名称:{} \t 项目位置:{} \t 合同编号:{} \t 电子监管号:{} \t 面积(公顷):{} \t 土地来源:{} \t 供地方式:{} \t 土地使用年限:{} \t 行业分类:{} \t 土地用途:{} \t 土地级别:{} \t 成交价格(万元):{} \t 土地使用权人:{} \t 约定容积率(下限):{} \t 约定容积率(上限):{} \t 约定交地时间:{} \t 约定开工时间:{} \t 约定竣工时间:{} \t 批准单位:{} \t 签订日期:{} \n'.format(
                comment['行政区'], comment['项目名称'], comment['项目位置'], comment['合同编号'],  comment['电子监管号'],comment['面积(公顷)'],comment['土地来源'],comment['供地方式'],comment['土地使用年限'] ,comment['行业分类'],comment['土地用途'],comment['土地级别'],comment['成交价格(万元)'] ,comment['土地使用权人'] ,comment['约定容积率(下限)'],comment['约定容积率(上限)'] ,comment['约定交地时间'] ,comment['约定开工时间'] ,comment['约定竣工时间'] ,comment['批准单位'],comment['签订日期'] ))
        print('当前页面爬取完成')


def main():
    with open('TTBT.txt', 'r') as f:
        count=0
        for line in f:
            count+=1
            print("正在解析第{}个页面，一共有4122个页面".format(count))
            time.sleep(1)
            values=get_content(line)
            outfile(values)
            print("所有信息都已经爬取完成")

if __name__ == '__main__':
    main()
```
最终就获取到了所有的数据。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200613165624512.png#pic_center)
#### 总结
这个网站比较简单，在爬取的过程中没有遇到什么大的困难。也就是两个点。第一点就是真实链接的寻找，第二点就是设置一个睡眠时间，防止爬取速度过快，网页没有加载出来。

下一篇会写爬取中国土地网的数据，这里面涉及到的难点就比较多了。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200613170340563.jpg#pic_center)

